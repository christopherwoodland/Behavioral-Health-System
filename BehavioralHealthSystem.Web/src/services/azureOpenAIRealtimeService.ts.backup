/**
 * Azure OpenAI Realtime WebRTC Service
 * Direct client-side connection to Azure OpenAI Realtime API
 * Based on Microsoft's TypeScript reference implementation:
 * https://learn.microsoft.com/en-us/azure/ai-foundry/openai/realtime-audio-quickstart
 * https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/realtime-audio-webrtc
 */

export interface RealtimeMessage {
  id: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: string;
  audioUrl?: string;
}

export interface RealtimeSessionConfig {
  enableAudio: boolean;
  enableVAD: boolean; // Voice Activity Detection
  temperature?: number;
  maxTokens?: number;
  voice?: 'alloy' | 'echo' | 'shimmer';
  instructions?: string;
}

export interface VoiceActivity {
  volumeLevel: number;
  isSpeaking: boolean;
  timestamp: number;
}

export interface SessionStatus {
  isConnected: boolean;
  isSessionActive: boolean;
  sessionId: string | null;
  connectionQuality: 'excellent' | 'good' | 'fair' | 'poor';
}

/**
 * Azure OpenAI Realtime WebRTC Service
 * Handles direct browser-to-Azure OpenAI connection via WebRTC
 */
export class AzureOpenAIRealtimeService {
  private peerConnection: RTCPeerConnection | null = null;
  private dataChannel: RTCDataChannel | null = null;
  private localStream: MediaStream | null = null;
  private remoteStream: MediaStream | null = null;
  private audioElement: HTMLAudioElement | null = null;
  
  private sessionId: string | null = null;
  private isConnected: boolean = false;
  private isSessionActive: boolean = false;
  
  private voiceActivityInterval: NodeJS.Timeout | null = null;
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private microphoneSource: MediaStreamAudioSourceNode | null = null;
  
  private endpoint: string;
  private apiKey: string;
  private deploymentName: string;
  private apiVersion: string;
  private webrtcRegion: string;
  private ephemeralKey: string = '';
  
  private messageHistory: RealtimeMessage[] = [];
  
  // Event callbacks
  private onMessageCallback: ((message: RealtimeMessage) => void) | null = null;
  private onVoiceActivityCallback: ((activity: VoiceActivity) => void) | null = null;
  private onStatusChangeCallback: ((status: SessionStatus) => void) | null = null;
  private onErrorCallback: ((error: Error) => void) | null = null;
  private onTranscriptCallback: ((transcript: string, isFinal: boolean) => void) | null = null;
  
  constructor() {
    // Load configuration from environment variables
    // Two-step authentication: sessions endpoint + WebRTC regional endpoint
    this.endpoint = import.meta.env.VITE_AZURE_OPENAI_RESOURCE_NAME || '';
    this.apiKey = import.meta.env.VITE_AZURE_OPENAI_REALTIME_KEY || '';
    this.deploymentName = import.meta.env.VITE_AZURE_OPENAI_REALTIME_DEPLOYMENT || 'gpt-realtime';
    this.apiVersion = import.meta.env.VITE_AZURE_OPENAI_REALTIME_API_VERSION || '2025-04-01-preview';
    this.webrtcRegion = import.meta.env.VITE_AZURE_OPENAI_WEBRTC_REGION || 'eastus2';

    console.log('üîß Azure OpenAI Realtime Config:');
    console.log('  Resource:', this.endpoint);
    console.log('  Deployment:', this.deploymentName);
    console.log('  API Version:', this.apiVersion);
    console.log('  WebRTC Region:', this.webrtcRegion);    if (!this.endpoint || !this.apiKey) {
      console.warn('‚ö†Ô∏è Azure OpenAI Realtime credentials not configured');
    }
  }
  
  /**
   * Event listeners
   */
  onMessage(callback: (message: RealtimeMessage) => void): void {
    this.onMessageCallback = callback;
  }
  
  onVoiceActivity(callback: (activity: VoiceActivity) => void): void {
    this.onVoiceActivityCallback = callback;
  }
  
  onStatusChange(callback: (status: SessionStatus) => void): void {
    this.onStatusChangeCallback = callback;
  }
  
  onError(callback: (error: Error) => void): void {
    this.onErrorCallback = callback;
  }
  
  onTranscript(callback: (transcript: string, isFinal: boolean) => void): void {
    this.onTranscriptCallback = callback;
  }
  
  /**
   * Initialize the service
   */
  async initialize(): Promise<void> {
    try {
      // Verify WebRTC support
      if (!('RTCPeerConnection' in window)) {
        throw new Error('WebRTC is not supported in this browser');
      }
      
      if (!('MediaStream' in window)) {
        throw new Error('MediaStream API is not supported in this browser');
      }
      
      this.isConnected = true;
      this.emitStatusChange();
      
      console.log('‚úÖ Azure OpenAI Realtime service initialized');
    } catch (error) {
      console.error('‚ùå Failed to initialize service:', error);
      this.handleError(error as Error);
      throw error;
    }
  }
  
  /**
   * Start a new realtime session with WebRTC
   */
  async startSession(userId: string, config: RealtimeSessionConfig): Promise<void> {
    try {
      if (!this.endpoint || !this.apiKey) {
        throw new Error('Azure OpenAI Realtime credentials not configured. Please set environment variables.');
      }
      
      console.log('üöÄ Starting Azure OpenAI Realtime session...');
      
      this.sessionId = `session-${userId}-${Date.now()}`;
      
      // Step 1: Get ephemeral key from sessions API
      console.log('üîë Getting ephemeral key from Azure OpenAI sessions API...');
      await this.getEphemeralKey(config);
      
      // Get user media (microphone) with selected device
      await this.initializeAudioStream(config.enableAudio);
      
      // Setup audio analysis for voice activity detection
      if (config.enableVAD && this.localStream) {
        this.setupVoiceActivityDetection();
      }
      
      // Create RTCPeerConnection
      await this.createPeerConnection(config);
      
      // Step 2: Establish WebRTC connection to Azure OpenAI with ephemeral key
      await this.connectToAzureOpenAI();
      
      this.isSessionActive = true;
      this.emitStatusChange();
      
      console.log('‚úÖ Session started successfully:', this.sessionId);
    } catch (error) {
      console.error('‚ùå Failed to start session:', error);
      this.handleError(error as Error);
      await this.cleanup();
      throw error;
    }
  }
  
  /**
   * Initialize audio stream from microphone
   */
  private async initializeAudioStream(enableAudio: boolean): Promise<void> {
    if (!enableAudio) {
      console.log('Audio disabled, skipping microphone initialization');
      return;
    }
    
    try {
      const constraints: MediaStreamConstraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000, // Azure OpenAI recommended sample rate
        },
        video: false
      };
      
      this.localStream = await navigator.mediaDevices.getUserMedia(constraints);
      console.log('üé§ Microphone access granted');
    } catch (error) {
      console.error('‚ùå Failed to access microphone:', error);
      throw new Error('Microphone access denied. Please grant microphone permissions.');
    }
  }
  
  /**
   * Setup voice activity detection
   */
  private setupVoiceActivityDetection(): void {
    if (!this.localStream) return;
    
    try {
      this.audioContext = new AudioContext({ sampleRate: 24000 });
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;
      
      this.microphoneSource = this.audioContext.createMediaStreamSource(this.localStream);
      this.microphoneSource.connect(this.analyser);
      
      // Start monitoring voice activity
      this.startVoiceActivityMonitoring();
      
      console.log('üéôÔ∏è Voice activity detection enabled');
    } catch (error) {
      console.error('Failed to setup voice activity detection:', error);
    }
  }
  
  /**
   * Monitor voice activity levels
   */
  private startVoiceActivityMonitoring(): void {
    if (!this.analyser) return;
    
    const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
    
    this.voiceActivityInterval = setInterval(() => {
      if (!this.analyser) return;
      
      this.analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume level
      const sum = dataArray.reduce((a, b) => a + b, 0);
      const average = sum / dataArray.length;
      const volumeLevel = average / 255; // Normalize to 0-1
      
      // Detect if speaking (threshold: 0.05)
      const isSpeaking = volumeLevel > 0.05;
      
      const activity: VoiceActivity = {
        volumeLevel,
        isSpeaking,
        timestamp: Date.now()
      };
      
      if (this.onVoiceActivityCallback) {
        this.onVoiceActivityCallback(activity);
      }
    }, 50); // Update every 50ms for smooth visualization
  }
  
  /**
   * Create RTCPeerConnection
   */
  private async createPeerConnection(_config: RealtimeSessionConfig): Promise<void> {
    const configuration: RTCConfiguration = {
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
      ],
      iceCandidatePoolSize: 10
    };
    
    this.peerConnection = new RTCPeerConnection(configuration);
    
    // Add local audio track
    if (this.localStream) {
      this.localStream.getTracks().forEach(track => {
        if (this.peerConnection && this.localStream) {
          this.peerConnection.addTrack(track, this.localStream);
          console.log('‚ûï Added local audio track');
        }
      });
    }
    
    // Handle remote tracks (AI audio responses)
    this.peerConnection.ontrack = (event) => {
      console.log('üì° Received remote track');
      this.remoteStream = event.streams[0];
      this.playRemoteAudio();
    };
    
    // Handle ICE candidates
    this.peerConnection.onicecandidate = (event) => {
      if (event.candidate) {
        console.log('üßä ICE candidate:', event.candidate.candidate);
      }
    };
    
    // Handle connection state changes
    this.peerConnection.onconnectionstatechange = () => {
      console.log('üîå Connection state:', this.peerConnection?.connectionState);
      this.emitStatusChange();
    };
    
    // Create data channel for realtime events (must be created BEFORE offer)
    // Using 'realtime-channel' name as per Azure OpenAI specification
    this.dataChannel = this.peerConnection.createDataChannel('realtime-channel', {
      ordered: true
    });
    
    this.dataChannel.onmessage = (event) => {
      this.handleDataChannelMessage(event.data);
    };
    
    this.dataChannel.onopen = () => {
      console.log('üì¨ Data channel is open');
      // Send session.update after data channel opens (as per working example)
      this.updateSession(_config);
    };
    
    this.dataChannel.onclose = () => {
      console.log('üì™ Data channel is closed');
    };
    
    console.log('‚úÖ RTCPeerConnection created');
  }
  
  /**
   * Connect to Azure OpenAI Realtime API
   */
  private async connectToAzureOpenAI(): Promise<void> {
    if (!this.peerConnection) {
      throw new Error('Peer connection not initialized');
    }
    
    try {
      // Create offer
      const offer = await this.peerConnection.createOffer({
        offerToReceiveAudio: true,
        offerToReceiveVideo: false
      });
      
      await this.peerConnection.setLocalDescription(offer);
      console.log('üì§ Created and set local offer');
      
      // Send offer to Azure OpenAI and get answer
      const answer = await this.exchangeSDPWithAzure(offer);
      
      await this.peerConnection.setRemoteDescription(new RTCSessionDescription(answer));
      console.log('üì• Set remote answer from Azure OpenAI');
      
    } catch (error) {
      console.error('‚ùå Failed to connect to Azure OpenAI:', error);
      throw error;
    }
  }
  
  /**
   * Get ephemeral key from Azure OpenAI sessions API (Step 1 of 2-step auth)
   */
  private async getEphemeralKey(config: RealtimeSessionConfig): Promise<void> {
    const sessionsUrl = `https://${this.endpoint}.openai.azure.com/openai/realtimeapi/sessions?api-version=${this.apiVersion}`;
    
    const payload = {
      model: this.deploymentName,
      voice: config.voice || 'alloy',
      instructions: config.instructions || 'You are a helpful behavioral health assistant.',
      temperature: config.temperature || 0.8,
      max_output_tokens: config.maxTokens || 4096
    };
    
    try {
      console.log('üì° Sessions API URL:', sessionsUrl);
      
      const response = await fetch(sessionsUrl, {
        method: 'POST',
        headers: {
          'api-key': this.apiKey,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(payload)
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Sessions API error: ${response.status} - ${errorText}`);
      }
      
      const data = await response.json();
      this.ephemeralKey = data.client_secret.value;
      
      console.log('‚úÖ Ephemeral key obtained successfully');
    } catch (error) {
      console.error('‚ùå Failed to get ephemeral key:', error);
      throw error;
    }
  }

  /**
   * Exchange SDP with Azure OpenAI Realtime API (Step 2 of 2-step auth)
   */
  private async exchangeSDPWithAzure(
    offer: RTCSessionDescriptionInit
  ): Promise<RTCSessionDescriptionInit> {
    // Use regional WebRTC endpoint with deployment as query parameter
    const webrtcUrl = `https://${this.webrtcRegion}.realtimeapi-preview.ai.azure.com/v1/realtimertc?model=${this.deploymentName}`;
    
    try {
      console.log('üì° WebRTC URL:', webrtcUrl);
      
      const response = await fetch(webrtcUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.ephemeralKey}`,
          'Content-Type': 'application/sdp'
        },
        body: offer.sdp
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`WebRTC API error: ${response.status} - ${errorText}`);
      }
      
      const sdpAnswer = await response.text();
      
      console.log('‚úÖ SDP answer received from Azure OpenAI');
      
      return {
        type: 'answer' as RTCSdpType,
        sdp: sdpAnswer
      };
    } catch (error) {
      console.error('‚ùå Failed to exchange SDP with Azure:', error);
      throw error;
    }
  }
  
  /**
   * Send session.update event to configure the session
   * Called automatically when data channel opens
   */
  private updateSession(config: RealtimeSessionConfig): void {
    if (!this.dataChannel || this.dataChannel.readyState !== 'open') {
      console.warn('‚ö†Ô∏è Data channel not open, cannot send session.update');
      return;
    }
    
    const event = {
      type: 'session.update',
      session: {
        instructions: config.instructions || 'You are a helpful behavioral health assistant responding in natural, engaging language.',
        voice: config.voice || 'alloy',
        temperature: config.temperature || 0.8
      }
    };
    
    try {
      this.dataChannel.send(JSON.stringify(event));
      console.log('üì§ Sent client event:', JSON.stringify(event, null, 2));
    } catch (error) {
      console.error('‚ùå Failed to send session.update:', error);
    }
  }
  
  /**
   * Play remote audio from AI
   */
  private playRemoteAudio(): void {
    if (!this.remoteStream) return;
    
    if (!this.audioElement) {
      this.audioElement = new Audio();
      this.audioElement.autoplay = true;
    }
    
    this.audioElement.srcObject = this.remoteStream;
    console.log('üîä Playing AI audio response');
  }
  
  /**
   * Handle data channel messages from Azure OpenAI
   * Handles various event types as per Azure OpenAI Realtime API specification
   */
  private handleDataChannelMessage(data: string): void {
    try {
      const realtimeEvent = JSON.parse(data);
      console.log('üì• Received server event:', realtimeEvent.type, realtimeEvent);
      
      // Handle different event types from Azure OpenAI
      if (realtimeEvent.type === 'session.update') {
        const instructions = realtimeEvent.session?.instructions;
        console.log('üìã Session updated. Instructions:', instructions);
      } else if (realtimeEvent.type === 'session.error') {
        console.error('‚ùå Session error:', realtimeEvent.error?.message);
        if (this.onErrorCallback) {
          this.onErrorCallback(new Error(realtimeEvent.error?.message || 'Session error'));
        }
      } else if (realtimeEvent.type === 'session.end') {
        console.log('üõë Session ended by server');
        this.cleanup();
      } else if (realtimeEvent.type === 'response.audio_transcript.delta') {
        // Handle transcript deltas (partial transcripts)
        if (this.onTranscriptCallback && realtimeEvent.delta) {
          this.onTranscriptCallback(realtimeEvent.delta, false);
        }
      } else if (realtimeEvent.type === 'response.audio_transcript.done') {
        // Handle completed transcripts
        if (this.onTranscriptCallback && realtimeEvent.transcript) {
          this.onTranscriptCallback(realtimeEvent.transcript, true);
        }
      } else if (realtimeEvent.type === 'response.done') {
        // Handle completed responses
        console.log('‚úÖ Response completed');
      } else if (realtimeEvent.type === 'transcript') {
        // Legacy handling for backward compatibility
        if (this.onTranscriptCallback) {
          this.onTranscriptCallback(realtimeEvent.text, realtimeEvent.isFinal);
        }
      } else if (realtimeEvent.type === 'response') {
        const realtimeMessage: RealtimeMessage = {
          id: `msg-${Date.now()}`,
          role: 'assistant',
          content: realtimeEvent.text || '',
          timestamp: new Date().toISOString()
        };
        
        this.messageHistory.push(realtimeMessage);
        
        if (this.onMessageCallback) {
          this.onMessageCallback(realtimeMessage);
        }
      }
    } catch (error) {
      console.error('Failed to parse data channel message:', error);
    }
  }
  
  /**
   * Send text message through data channel
   */
  async sendTextMessage(text: string): Promise<void> {
    if (!this.dataChannel || this.dataChannel.readyState !== 'open') {
      throw new Error('Data channel is not open');
    }
    
    const message = {
      type: 'message',
      text,
      timestamp: new Date().toISOString()
    };
    
    this.dataChannel.send(JSON.stringify(message));
    
    // Add to message history
    const userMessage: RealtimeMessage = {
      id: `msg-${Date.now()}`,
      role: 'user',
      content: text,
      timestamp: new Date().toISOString()
    };
    
    this.messageHistory.push(userMessage);
    
    if (this.onMessageCallback) {
      this.onMessageCallback(userMessage);
    }
  }
  
  /**
   * Change microphone device
   */
  async changeMicrophone(deviceId: string): Promise<void> {
    try {
      // Stop current stream
      if (this.localStream) {
        this.localStream.getTracks().forEach(track => track.stop());
      }
      
      // Get new stream with selected device
      const constraints: MediaStreamConstraints = {
        audio: {
          deviceId: { exact: deviceId },
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000
        }
      };
      
      this.localStream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Update peer connection tracks
      if (this.peerConnection) {
        const sender = this.peerConnection.getSenders().find(s => s.track?.kind === 'audio');
        if (sender && this.localStream) {
          const audioTrack = this.localStream.getAudioTracks()[0];
          await sender.replaceTrack(audioTrack);
          console.log('üé§ Microphone changed successfully');
        }
      }
      
      // Restart voice activity detection
      if (this.analyser) {
        this.setupVoiceActivityDetection();
      }
    } catch (error) {
      console.error('Failed to change microphone:', error);
      throw error;
    }
  }
  
  /**
   * Pause session (mute microphone)
   */
  pauseSession(): void {
    if (this.localStream) {
      this.localStream.getAudioTracks().forEach(track => {
        track.enabled = false;
      });
      console.log('‚è∏Ô∏è Session paused');
      this.emitStatusChange();
    }
  }
  
  /**
   * Resume session (unmute microphone)
   */
  resumeSession(): void {
    if (this.localStream) {
      this.localStream.getAudioTracks().forEach(track => {
        track.enabled = true;
      });
      console.log('‚ñ∂Ô∏è Session resumed');
      this.emitStatusChange();
    }
  }
  
  /**
   * End session and cleanup
   */
  async endSession(): Promise<void> {
    console.log('üõë Ending session...');
    await this.cleanup();
    this.isSessionActive = false;
    this.sessionId = null;
    this.emitStatusChange();
    console.log('‚úÖ Session ended');
  }
  
  /**
   * Cleanup resources
   */
  private async cleanup(): Promise<void> {
    // Stop voice activity monitoring
    if (this.voiceActivityInterval) {
      clearInterval(this.voiceActivityInterval);
      this.voiceActivityInterval = null;
    }
    
    // Close audio context
    if (this.audioContext) {
      await this.audioContext.close();
      this.audioContext = null;
    }
    
    // Stop local stream
    if (this.localStream) {
      this.localStream.getTracks().forEach(track => track.stop());
      this.localStream = null;
    }
    
    // Close data channel
    if (this.dataChannel) {
      this.dataChannel.close();
      this.dataChannel = null;
    }
    
    // Close peer connection
    if (this.peerConnection) {
      this.peerConnection.close();
      this.peerConnection = null;
    }
    
    // Stop remote audio
    if (this.audioElement) {
      this.audioElement.pause();
      this.audioElement.srcObject = null;
      this.audioElement = null;
    }
    
    this.remoteStream = null;
    this.analyser = null;
    this.microphoneSource = null;
  }
  
  /**
   * Get session status
   */
  getStatus(): SessionStatus {
    let connectionQuality: 'excellent' | 'good' | 'fair' | 'poor' = 'poor';
    
    if (this.peerConnection) {
      const state = this.peerConnection.connectionState;
      if (state === 'connected') connectionQuality = 'excellent';
      else if (state === 'connecting') connectionQuality = 'good';
      else if (state === 'new') connectionQuality = 'fair';
    }
    
    return {
      isConnected: this.isConnected,
      isSessionActive: this.isSessionActive,
      sessionId: this.sessionId,
      connectionQuality
    };
  }
  
  /**
   * Get message history
   */
  getMessageHistory(): RealtimeMessage[] {
    return [...this.messageHistory];
  }
  
  /**
   * Clear message history
   */
  clearHistory(): void {
    this.messageHistory = [];
  }
  
  /**
   * Emit status change event
   */
  private emitStatusChange(): void {
    if (this.onStatusChangeCallback) {
      this.onStatusChangeCallback(this.getStatus());
    }
  }
  
  /**
   * Handle errors
   */
  private handleError(error: Error): void {
    if (this.onErrorCallback) {
      this.onErrorCallback(error);
    }
  }
  
  /**
   * Destroy service instance
   */
  async destroy(): Promise<void> {
    await this.cleanup();
    this.isConnected = false;
    this.onMessageCallback = null;
    this.onVoiceActivityCallback = null;
    this.onStatusChangeCallback = null;
    this.onErrorCallback = null;
    this.onTranscriptCallback = null;
    console.log('üóëÔ∏è Service destroyed');
  }
}

// Export singleton instance
export const azureOpenAIRealtimeService = new AzureOpenAIRealtimeService();
