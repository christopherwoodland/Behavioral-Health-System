# Independent User/Agent Handler Architecture

## Overview
The Azure OpenAI Realtime service has been refactored to separate user input and agent response processing into independent handler classes. This improves code organization, testability, and maintains clear responsibility boundaries while sharing the necessary WebRTC connection.

## Architecture

### Core Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         AzureOpenAIRealtimeService (Main Service)           ‚îÇ
‚îÇ  - WebRTC Connection (RTCPeerConnection)                    ‚îÇ
‚îÇ  - Data Channel (RTCDataChannel)                            ‚îÇ
‚îÇ  - Event Routing                                            ‚îÇ
‚îÇ  - Session Management                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                      ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  UserInputHandler  ‚îÇ ‚îÇ AgentResponseHandler‚îÇ
     ‚îÇ                    ‚îÇ ‚îÇ                     ‚îÇ
     ‚îÇ  - VAD             ‚îÇ ‚îÇ  - Response Tracking‚îÇ
     ‚îÇ  - User Transcripts‚îÇ ‚îÇ  - Agent Transcripts‚îÇ
     ‚îÇ  - Speech Detection‚îÇ ‚îÇ  - Mic Muting       ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### UserInputHandler
**Purpose:** Manages all user voice input, voice activity detection, and user transcription

**Responsibilities:**
- ‚úÖ Voice Activity Detection (VAD)
  - AudioContext analysis
  - Volume monitoring (50ms intervals)
  - Speech start/stop detection
- ‚úÖ User transcript processing
  - Handles `conversation.item.input_audio_transcription.completed`
- ‚úÖ Speech detection state (user side)
- ‚úÖ Independent audio analysis pipeline

**Key Methods:**
- `initializeVAD(stream: MediaStream)` - Setup VAD on user's audio
- `handleTranscriptEvent(event)` - Process user transcripts
- `isCurrentlySpeaking()` - Get current user speaking state
- `cleanup()` - Release resources

**Callbacks:**
- `onVoiceActivity(callback)` - Volume/speaking updates
- `onLiveTranscript(callback)` - User transcript updates
- `onSpeechDetection(callback)` - Speech state changes

### AgentResponseHandler
**Purpose:** Manages all agent voice responses, transcription, and microphone muting

**Responsibilities:**
- ‚úÖ Response lifecycle tracking
  - `response.created` - Agent about to speak
  - `response.done` - Agent finished
  - `response.cancelled` - Response interrupted
- ‚úÖ Agent transcript processing
  - `response.audio_transcript.delta` - Streaming text
  - `response.audio_transcript.done` - Final transcript
- ‚úÖ Microphone muting strategy
  - Mute BEFORE agent speaks (`response.created`)
  - Unmute 1.5s AFTER agent finishes (`response.audio_transcript.done`)
  - Immediate unmute on cancellation
- ‚úÖ Message history management

**Key Methods:**
- `handleResponseCreated(event)` - Agent starting to speak
- `handleAudioTranscriptDelta(event)` - Streaming agent text
- `handleAudioTranscriptDone(event)` - Agent finished speaking
- `handleResponseDone(event)` - Response generation complete
- `handleResponseCancelled(event)` - Response interrupted
- `isCurrentlySpeaking()` - Get current agent speaking state
- `cleanup()` - Release resources

**Callbacks:**
- `onLiveTranscript(callback)` - Agent transcript updates
- `onMessage(callback)` - Complete messages
- `onSpeechDetection(callback)` - Speech state changes
- `onMuteMicrophone(callback)` - Mic mute/unmute requests

## Event Flow

### User Speaking Flow
```
1. User starts speaking
   ‚Üì
2. UserInputHandler.VAD detects volume increase
   ‚Üì
3. onVoiceActivity(callback) fires
   ‚Üì
4. Azure OpenAI detects speech: input_audio_buffer.speech_started
   ‚Üì
5. User stops speaking
   ‚Üì
6. UserInputHandler.VAD detects volume decrease
   ‚Üì
7. Azure OpenAI transcribes: conversation.item.input_audio_transcription.completed
   ‚Üì
8. UserInputHandler.handleTranscriptEvent() processes transcript
   ‚Üì
9. Main service receives user transcript via callback
```

### Agent Speaking Flow
```
1. response.created event
   ‚Üì
2. AgentResponseHandler.handleResponseCreated()
   ‚Üì
3. üîá Microphone MUTED immediately (via callback to main service)
   ‚Üì
4. response.audio_transcript.delta events (streaming)
   ‚Üì
5. AgentResponseHandler.handleAudioTranscriptDelta()
   ‚Üì
6. Agent speaks (microphone stays muted)
   ‚Üì
7. response.audio_transcript.done
   ‚Üì
8. AgentResponseHandler.handleAudioTranscriptDone()
   ‚Üì
9. Wait 1.5 seconds
   ‚Üì
10. üîä Microphone UNMUTED (via callback to main service)
    ‚Üì
11. response.done event
    ‚Üì
12. AgentResponseHandler.handleResponseDone()
```

## Integration Points

### Main Service Responsibilities
- ‚úÖ Manages WebRTC connection (RTCPeerConnection)
- ‚úÖ Manages data channel (RTCDataChannel)
- ‚úÖ Routes events to appropriate handlers
- ‚úÖ Provides microphone control to handlers
- ‚úÖ Maintains backward compatibility with existing callbacks
- ‚úÖ Session management (start/stop/pause)
- ‚úÖ Function calling integration

### Handler Integration
```typescript
constructor() {
  // Initialize handlers
  this.userInputHandler = new UserInputHandler();
  this.agentResponseHandler = new AgentResponseHandler();
  
  // Connect agent handler's mute callback
  this.agentResponseHandler.onMuteMicrophone((mute) => {
    this.muteMicrophone(mute);
  });
  
  // Forward handler events to main service callbacks
  this.agentResponseHandler.onMessage((message) => {
    if (this.onMessageCallback) {
      this.onMessageCallback(message);
    }
  });
  
  this.userInputHandler.onVoiceActivity((activity) => {
    if (this.onVoiceActivityCallback) {
      this.onVoiceActivityCallback(activity);
    }
  });
}
```

### Event Routing
```typescript
private handleDataChannelMessage(data: string): void {
  const realtimeEvent = JSON.parse(data);
  
  // Route to appropriate handler
  switch (realtimeEvent.type) {
    // User events ‚Üí UserInputHandler
    case 'conversation.item.input_audio_transcription.completed':
      this.userInputHandler.handleTranscriptEvent(realtimeEvent);
      break;
    
    // Agent events ‚Üí AgentResponseHandler
    case 'response.created':
      this.agentResponseHandler.handleResponseCreated(realtimeEvent);
      break;
      
    case 'response.audio_transcript.delta':
      this.agentResponseHandler.handleAudioTranscriptDelta(realtimeEvent);
      break;
      
    case 'response.audio_transcript.done':
      this.agentResponseHandler.handleAudioTranscriptDone(realtimeEvent);
      break;
      
    case 'response.done':
      this.agentResponseHandler.handleResponseDone(realtimeEvent);
      break;
      
    case 'response.cancelled':
      this.agentResponseHandler.handleResponseCancelled(realtimeEvent);
      break;
      
    // Session/error events stay in main service
    case 'session.created':
    case 'session.update':
    case 'error':
      // Handle in main service
      break;
  }
}
```

## Benefits

### ‚úÖ Separation of Concerns
- User input logic isolated from agent response logic
- Each handler has clear, focused responsibilities
- Easier to understand and maintain

### ‚úÖ Independent State Management
- User handler tracks: `isUserSpeaking`, `vadStartTime`, `vadEndTime`
- Agent handler tracks: `isAISpeaking`, `responseStartTime`, `responseEndTime`
- No state conflicts between user and agent processing

### ‚úÖ Easier Testing
- Can unit test UserInputHandler independently
- Can unit test AgentResponseHandler independently
- Mock callbacks for isolated testing

### ‚úÖ Better Code Organization
- ~200 lines each handler vs 1500+ line monolithic service
- Clear file structure: `services/handlers/`
- Easy to locate specific functionality

### ‚úÖ Backward Compatibility
- All existing callbacks still work
- Main service forwards handler events to legacy callbacks
- No breaking changes to external API

## Constraints

### ‚ùå Shared WebRTC Connection
- Single `RTCPeerConnection` for both directions
- Cannot create separate connections
- Protocol limitation, not code limitation

### ‚ùå Single Data Channel
- All events come through one `DataChannel.onmessage`
- Main service must route events
- Cannot create separate channels per handler

### ‚ùå Microphone Control
- Microphone muting affects both handlers
- Agent handler requests mute via callback
- Main service executes mute (owns MediaStream)

## File Structure

```
BehavioralHealthSystem.Web/src/services/
‚îú‚îÄ‚îÄ azureOpenAIRealtimeService.ts          (Main service - 1500 lines)
‚îî‚îÄ‚îÄ handlers/
    ‚îú‚îÄ‚îÄ UserInputHandler.ts                (User handler - ~180 lines)
    ‚îî‚îÄ‚îÄ AgentResponseHandler.ts            (Agent handler - ~280 lines)
```

## Migration Notes

### What Changed
- VAD setup moved from main service to UserInputHandler
- Agent response handling moved to AgentResponseHandler
- Event routing logic added to main service
- Handlers initialized in constructor
- Cleanup updated to clean handlers

### What Stayed the Same
- Public API of main service (no breaking changes)
- WebRTC connection setup
- Session management
- Function calling
- All external callbacks still work

### Deprecated
- `startVoiceActivityMonitoring()` - Now in UserInputHandler
- Direct VAD interval management - Delegated to handler

## Testing Checklist

- [ ] VAD still detects user speech
- [ ] User transcripts appear correctly
- [ ] Agent responses play correctly
- [ ] Agent transcripts appear correctly
- [ ] Microphone mutes BEFORE agent speaks
- [ ] Microphone unmutes 1.5s AFTER agent finishes
- [ ] Response cancellation unmutes immediately
- [ ] Function calling still works
- [ ] Session pause/resume works
- [ ] PHQ assessments still trigger
- [ ] Cleanup releases all resources
- [ ] No memory leaks (check timeout cleanup)

## Future Enhancements

### Potential Improvements
1. **Separate VAD Configuration**
   - Allow different thresholds for user vs agent detection
   - Independent sensitivity settings

2. **Enhanced Agent Handler**
   - Track multiple concurrent responses
   - Support response queuing
   - Advanced muting strategies

3. **User Handler Extensions**
   - Noise profile learning
   - Speaker identification
   - Custom VAD algorithms

4. **Testing Infrastructure**
   - Mock handlers for testing
   - Handler-level unit tests
   - Integration test suite

5. **Performance Monitoring**
   - Handler-specific metrics
   - Performance profiling per handler
   - Resource usage tracking

## Conclusion

The separated handler architecture provides a clean, maintainable structure for managing user input and agent responses independently while sharing the necessary WebRTC infrastructure. This design improves code quality without sacrificing functionality or backward compatibility.
